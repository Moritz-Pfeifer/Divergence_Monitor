{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2GZUcgJHIDfe"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = '/content/drive/MyDrive/Leipzig/Divergence_Indicator_2_0/Data/Data_Final'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFRTibP_h7v5","executionInfo":{"status":"ok","timestamp":1730371503654,"user_tz":-60,"elapsed":20891,"user":{"displayName":"Moritz Pfeifer","userId":"03033656427288486376"}},"outputId":"4083f353-bf70-4238-db00-ff9f58a82005"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import statsmodels.api as sm\n","from scipy.signal import convolve\n","\n","def vech(x):\n","    \"\"\"\n","    Transform a square matrix into a vector containing the lower triangular part.\n","    Returns a vector of length k*(k+1)/2 containing elements from the lower triangular part of x.\n","    \"\"\"\n","    #return x[np.tril_indices(x.shape[0])]\n","    rows, cols = x.shape\n","    return np.hstack([x[i:, i] for i in range(cols)])\n","\n","def ivech(data):\n","    \"\"\"\n","    Transform a vector into a lower triangular matrix.\n","    Returns a k by k lower triangular matrix.\n","    \"\"\"\n","    t = len(data)\n","    # Calculate the size of the output matrix\n","    sizeout = int((-1 + np.sqrt(1 + 8 * t)) / 2)\n","    transformeddata = np.zeros((sizeout, sizeout))\n","\n","    index = 0\n","    for i in range(sizeout):\n","        for j in range(i, sizeout):\n","            transformeddata[j, i] = data[index]\n","            index += 1\n","\n","    return transformeddata\n","\n","def myfun(x, FCyc_s, FCyc_us):\n","    \"\"\"\n","    Objective function for optimization, calculating the sum of squared differences.\n","    Returns the sum of squared differences between the modeled and actual data.\n","    \"\"\"\n","    return np.sum((FCyc_s * x[0] + x[1] - FCyc_us) ** 2)\n","\n","def fn_forecast_ar(temp, p_min=1, p_max=8): # Tested and works\n","    \"\"\"\n","    Forecast future values of a time series using an autoregressive (AR) model.\n","    Returns:\n","    - dY_w_fcast (numpy.ndarray): The original series with the forecasted values appended.\n","    - length_fcast (int): The number of forecasted values.\n","    \"\"\"\n","    # Determine the number of forecasts to make\n","    T = len(temp)\n","    length_fcast = min(int(2 * T / 3), 40)\n","\n","    # Fit the AR model within the specified lag range\n","    model = sm.tsa.arima.ARIMA(temp, order=(p_max, 0, 0), trend='n')\n","    results = model.fit()\n","\n","    # Forecast future values\n","    forecast = results.forecast(steps=length_fcast)\n","\n","    # Combine the original data with the forecasted values\n","    dY_w_fcast = np.concatenate([temp, forecast])\n","\n","    return dY_w_fcast, length_fcast\n","\n","def undrift(x, T, nvars):\n","    \"\"\"Removes drift from the data.\"\"\"\n","    xun = np.zeros_like(x)\n","    dd = np.arange(T)[:, None]\n","    for ivar in range(nvars):\n","        drift = (x[-1, ivar] - x[0, ivar]) / (T - 1)\n","        xun[:, ivar] = x[:, ivar] - dd * drift\n","    return xun\n","\n","def fn_bpass_all(X, pl, pu, root=0, drift=0, ifilt=0, nfix=-1, thet=1): # Tested and works\n","    # Calculate the angular frequencies for pl and pu\n","    b = 2 * np.pi / pl\n","    a = 2 * np.pi / pu\n","\n","    T = len(X)\n","\n","    # Prepare the theta array and compute the convolution to create the g array\n","    if not isinstance(thet, np.ndarray):\n","        thet = np.array([thet])\n","    nq = len(thet) - 1\n","    thet_padded = np.pad(thet, (0, nq), 'constant')\n","    g = convolve(thet, thet_padded[::-1], 'full')\n","    cc = g[nq:2*nq+1]\n","\n","    # Construct the ideal response matrix B\n","    j = np.arange(1, 2 * T + 1)\n","    B = np.hstack(((b - a) / np.pi, (np.sin(j * b) - np.sin(j * a)) / (j * np.pi)))\n","\n","    # Compute R using the closed-form integral solution\n","    R = np.zeros(T)\n","    R0 = B[0] * cc[0] + 2 * np.dot(B[1:nq+1], cc[1:nq+1])\n","    R[0] = np.pi * R0\n","    for i in range(1, T):\n","        R[i] = R[i - 1] - 2 * np.pi * B[i] * cc[0]\n","\n","    # Initialize the AA matrix\n","    AA = np.zeros((T, T))\n","\n","    if ifilt == 0:  # asymmetric filter\n","        for i in range(T):\n","            AA[i, i:T] = B[:T-i]\n","            if root == 1:\n","                AA[i, T-1] = R[T-i] / (2 * np.pi)\n","\n","        # Use symmetry to construct the bottom half of the AA matrix\n","        AA[0, 0] = AA[-1, -1]\n","        AA = AA + np.triu(AA, 1).T\n","\n","    if drift > 0:\n","        X = undrift(X, T, nvars)\n","\n","    # Apply the filter matrix AA to the data matrix X\n","    fX = np.dot(AA, X)\n","\n","    return fX"],"metadata":{"id":"fIqqtvDkJqpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fn_grid(q_low, q_high, resolution):\n","    \"\"\"\n","    Produces the grid along which the spectral density is estimated.\n","    Step defines the x-axis for graphs.\n","    \"\"\"\n","    omega_step = 2048\n","    pi = np.pi\n","\n","    if q_low == 0 and q_high != 0:\n","        grid = np.arange(2 * pi / q_high, pi, 2 * pi / omega_step)\n","    elif q_low != 0 and q_high == 0:\n","        grid = np.arange(0, (2 * pi) / q_low, 2 * pi / omega_step)\n","    elif q_low == 0 and q_high == 0:\n","        grid = np.arange(0, pi, 2 * pi / omega_step)\n","    else:\n","        grid = np.arange(2 * pi / q_high, (2 * pi) / q_low, 2 * pi / omega_step)\n","\n","    step = (max(grid) - min(grid)) / resolution  # define steps for frequency display in graphs\n","    return grid, step"],"metadata":{"id":"P3-oJVboJwrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.signal import windows, correlate\n","from numpy.fft import fft, ifft\n","\n","def parzenwin(window_len):\n","    \"\"\"Generate a Parzen window for given window length\"\"\"\n","    return windows.parzen(window_len)\n","\n","def pw_cohesion(dY, grid, factor, idx):\n","    dy = dY[:, np.array(idx, dtype=bool)]\n","    M = dy.shape[1]\n","\n","    f_hat_save = np.zeros(len(grid))\n","    f_hat_save_separate = []\n","\n","    kk = 0\n","    for ii in range(M - 1):\n","        for jj in range(ii + 1, M):\n","            kk += 1\n","            y = dy[:, ii]\n","            x = dy[:, jj]\n","\n","            # Clean missing or invalid data points\n","            valid = ~(np.isnan(x) | np.isnan(y))\n","            x = x[valid]\n","            y = y[valid]\n","\n","            T = len(y)\n","            gammahat = correlate(y, x, mode='full') / T\n","            gammahat /= (np.std(y) * np.std(x))\n","            t_cov = len(gammahat)\n","            Mwind = round(np.sqrt(len(y)) * factor)\n","            w = parzenwin(2 * Mwind + 1)\n","            ti = t_cov // 2\n","            tt = np.arange(ti + Mwind, ti - Mwind - 1, -1)\n","            cov_trunc = gammahat[tt] * w\n","\n","            # Fourier Transform\n","            f_hat = (2 / (2 * np.pi)) * np.abs(np.dot(cov_trunc, np.exp(-1j * np.arange(-Mwind, Mwind + 1)[:, None] * grid)))\n","\n","            f_hat_save += f_hat\n","            f_hat_save_separate.append(f_hat * (np.std(y) * np.std(x)))\n","\n","    f_hat_save /= kk\n","\n","    return f_hat_save, np.array(f_hat_save_separate)\n","\n","\n","def extrema(in_data):\n","    dsize = len(in_data)\n","    flag = 1\n","    spmax = [[1, in_data[0]]]\n","    spmin = [[1, in_data[0]]]\n","\n","    # Find local maxima\n","    for jj in range(1, dsize-1):\n","        if in_data[jj-1] <= in_data[jj] >= in_data[jj+1]:\n","            spmax.append([jj + 1, in_data[jj]])\n","        if in_data[jj-1] >= in_data[jj] <= in_data[jj+1]:\n","            spmin.append([jj + 1, in_data[jj]])\n","\n","    # Handle end points for maxima and minima\n","    spmax.append([dsize, in_data[-1]])\n","    spmin.append([dsize, in_data[-1]])\n","\n","    # Handle spline end effects if there are enough points\n","    if len(spmax) >= 4:\n","        adjust_endpoint(spmax)\n","    if len(spmin) >= 4:\n","        adjust_endpoint(spmin, mode='min')\n","\n","    return np.array(spmax), np.array(spmin), flag\n","\n","def adjust_endpoint(sp, mode='max'):\n","    # Adjust end point based on slope\n","    slope1 = (sp[1][1] - sp[2][1]) / (sp[1][0] - sp[2][0])\n","    tmp1 = slope1 * (sp[0][0] - sp[1][0]) + sp[1][1]\n","    if mode == 'max' and tmp1 > sp[0][1]:\n","        sp[0][1] = tmp1\n","    elif mode == 'min' and tmp1 < sp[0][1]:\n","        sp[0][1] = tmp1\n","\n","    slope2 = (sp[-2][1] - sp[-3][1]) / (sp[-2][0] - sp[-3][0])\n","    tmp2 = slope2 * (sp[-1][0] - sp[-2][0]) + sp[-2][1]\n","    if mode == 'max' and tmp2 > sp[-1][1]:\n","        sp[-1][1] = tmp2\n","    elif mode == 'min' and tmp2 < sp[-1][1]:\n","        sp[-1][1] = tmp2\n","\n"],"metadata":{"id":"Pfy77Jx5J0AO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.integrate import trapezoid\n","\n","def fn_integral_min_dist(coh, peak, grid, pct):\n","    \"\"\"Computes the percentage of the area below the spectral density with minimal distance\"\"\"\n","\n","    total_area = trapezoid(coh)\n","\n","    output_summary = np.zeros((len(grid)-1, 6))\n","    for ii in range(len(grid)-1):\n","        jj = 1\n","        share = trapezoid(coh[ii:ii+jj+1])\n","        while share / total_area <= pct and ii + jj != len(grid) - 1:\n","            jj += 1\n","            share = trapezoid(coh[ii:ii+jj+1])\n","\n","        output_summary[ii, :] = [ii, jj + ii, (grid[ii])**(-1) * np.pi/2, (grid[jj + ii])**(-1) * np.pi/2, jj, share / total_area]\n","\n","    # Applying filters to find the valid ranges according to the specified conditions\n","    temp = output_summary[output_summary[:, -1] > pct]  # filter via the pct rule\n","    temp = temp[temp[:, 2] >= peak]  # ensure peak is included\n","    temp = temp[temp[:, 3] <= peak]  # ensure peak is included\n","\n","    # Select the entry with the minimum distance in grid points\n","    min_distance = temp[temp[:, 4] == min(temp[:, 4])]\n","\n","    if len(min_distance) > 0:\n","        min_distance = min_distance[-1, :]  # taking the one with the highest frequency (or last in array if tied)\n","    else:\n","        min_distance = None  # handle case where no valid segment is found\n","\n","    return min_distance"],"metadata":{"id":"_lyCa9XTJ85H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import norm\n","from statsmodels.distributions.empirical_distribution import ECDF\n","from scipy.optimize import minimize\n","from numpy import nanmean, nanstd\n","\n","def fn_cycle_extract_ecdf_aggr(dY, idx, min_distance, corr_rol, sgn_res):\n","    d = sum(idx)\n","    idx = np.array(idx, dtype=bool)\n","    y = dY[:, idx]\n","    temp = np.full_like(y, np.nan)\n","    fdY = np.full_like(y, np.nan)\n","    temp_indicator_smooth = np.full_like(y, np.nan)\n","    FCyc_sav_unsmooth = np.full((y.shape[0], 1), np.nan)\n","    FCycle = np.full((y.shape[0], 1), np.nan)\n","    weights = np.full_like(y, np.nan)\n","    C_out = np.full((y.shape[0], int(d*(d+1)/2)), np.nan)\n","\n","    for ii in range(d):\n","        valid_indices = ~np.isnan(y[:, ii])\n","        temp2 = y[valid_indices, ii]\n","        temp2_fcast, length_fcast = fn_forecast_ar(temp2)\n","\n","        temp10 = fn_bpass_all(temp2_fcast - np.mean(temp2_fcast), min_distance[3]*4, min_distance[2]*4,0,0,0)\n","        temp10 = temp10[:len(temp10) - length_fcast] + np.mean(temp2)\n","\n","        # ECDF mapping\n","        ecdf_function = ECDF(temp2)\n","        xx = ecdf_function.x[1:]\n","        yy = ecdf_function.y[1:]\n","\n","        # Find unique values and indices\n","        unique_values, li = np.unique(temp2, return_inverse=True)\n","        idx_nan = np.isnan(temp[:, ii])\n","\n","        temp[idx_nan, ii] = yy[li]\n","        # Assign 'temp10' to 'fdY' where 'idx_nan' is True\n","        fdY[idx_nan, ii] = temp10\n","\n","        # Optimization\n","        x0 = [1, 0]\n","        res = minimize(lambda x: myfun(x, temp10, yy[li]), x0, method='L-BFGS-B',\n","                       options={'maxfun': 50000, 'maxiter': 50000, 'eps': 1.0e-5, 'ftol': 1.0e-5, 'disp': False})\n","        x = res.x\n","        temp_adjust = temp10 * x[0] + x[1]\n","\n","        temp_adjust[temp_adjust > 1] = 1  # Cut off max and min\n","        temp_adjust[temp_adjust < 0] = 0\n","\n","        nan_indices = np.where(idx_nan)[0]\n","        col_index = [ii]\n","        temp_indicator_smooth[nan_indices, col_index] = temp_adjust\n","\n","    dY_ecdf = temp\n","    idx_cyc_sav = np.all(~np.isnan(temp), axis=1)\n","    temp = temp[idx_cyc_sav, :]\n","    C = np.ones((d, d, len(temp)))\n","    C_vech = np.ones((len(temp), d * (d + 1) // 2))\n","    FCyc_sav = np.full((len(temp), 1), np.nan)\n","    weights_sav = np.full((len(temp), d), np.nan)\n","\n","    if corr_rol == 1:\n","        lam = 0.89  # Smoothing parameter\n","        sigma_ij = np.zeros((len(temp) + 1, d * (d + 1) // 2))\n","        # Initialize covariance using the first 8 rows and vectorize the lower triangular part\n","        initial_cov = np.nan_to_num(np.cov(temp[:8, :], rowvar=False))\n","        sigma_ij[0, :] = vech(initial_cov)\n","\n","        for ii in range(len(temp)):\n","            ttt = 0\n","            sgn_sigma = np.zeros((1, int(d * (d + 1) / 2)))\n","            for jj in range(d):\n","              for kk in range(jj, d):\n","                  ttt += 1\n","                  sigma_ij[ii + 1, ttt - 1] = lam * sigma_ij[ii, ttt - 1] + (1 - lam) * (temp[ii, jj] - 0.5) * (temp[ii, kk] - 0.5)\n","                  if sigma_ij[ii + 1, ttt - 1] > 0:\n","                    sgn_sigma[0, ttt - 1] = sigma_ij[ii + 1, ttt - 1]\n","\n","            if sgn_res == 1:\n","                temp_vech = ivech(sgn_sigma.flatten())  # Convert the row matrix to a vector\n","            else:\n","                temp_vech = ivech(sigma_ij[ii + 1, :])\n","\n","        # Compute covariance and correlation matrices\n","            temp_cov = temp_vech + temp_vech.T - np.diag(np.diag(temp_vech)) # Covariance\n","            temp_sqrt_diag = np.linalg.inv(np.sqrt(np.diag(np.diag(temp_vech)))) # Correlation\n","            C[:, :, ii - 1] = temp_sqrt_diag @ temp_cov @ temp_sqrt_diag\n","            C_vech[ii - 1, :] = vech(np.squeeze(C[:, :, ii - 1]))\n","            FCyc_sav[ii - 1, 0] = (np.sum(np.squeeze(C[:, :, ii - 1]) * temp[ii])) / np.sum(np.sum(np.squeeze(C[:, :, ii - 1]))) # Correlation weighted indices\n","            weights_sav[ii - 1] = np.sum(np.squeeze(C[:, :, ii - 1]), axis=0) / np.sum(np.sum(np.squeeze(C[:, :, ii - 1])))\n","        weights[idx_cyc_sav, :] = weights_sav\n","\n","    else:\n","        FCyc_sav = np.mean(temp, axis=1)\n","        #return FCyc_sav, weights\n","\n","    # Using ECDF on the filtered data\n","    temp = FCyc_sav\n","    ecdf_function = ECDF(temp.flatten())\n","    xx = ecdf_function.x[1:]  # The unique sorted values\n","    ff = ecdf_function.y[1:]  # The ECDF values at those points\n","\n","    # Mapping unique values from 'temp' to corresponding ECDF values\n","    unique_temp, li = np.unique(temp, return_inverse=True)\n","    FCyc_sav_unsmooth[idx_cyc_sav, 0] = ff[li]\n","\n","    temp_fcast, length_fcast = fn_forecast_ar(ff[li])\n","\n","    # Band-pass filtering the forecasted data\n","    temp_fcast_mean_adjusted = temp_fcast - np.mean(temp_fcast)\n","    FCyc_sav_smooth = fn_bpass_all(temp_fcast_mean_adjusted, min_distance[3] * 4, min_distance[2] * 4, 0, 0, 0)\n","\n","    # Calculating the mean of the adjusted ECDF values\n","    xx_mu = np.mean(ff[li])\n","\n","    # Adjusting the smooth data by adding the mean back to the filtered data, truncated by the forecast length\n","    FCyc_sav_smooth = FCyc_sav_smooth[:-length_fcast] + xx_mu\n","\n","    # Optimization\n","    x0 = [1, 0]\n","    res = minimize(lambda x: myfun(x, FCyc_sav_smooth, ff[li]), x0, method='L-BFGS-B',\n","                   options={'maxfun': 50000, 'maxiter': 50000, 'eps': 1.0e-5, 'ftol': 1.0e-5, 'disp': False})\n","    x = res.x\n","    temp_adjust = FCyc_sav_smooth * x[0] + x[1]\n","\n","    temp_adjust = np.clip(temp_adjust, 0, 1)\n","\n","    # Assigning the adjusted values to FCycle based on index\n","    FCycle[idx_cyc_sav] = temp_adjust.reshape(-1, 1)\n","\n","    # Mapping C_out values based on the index\n","    C_out[idx_cyc_sav, :] = C_vech[idx_cyc_sav, :]\n","\n","    # Constructing series_graph from FCycle and temp_indicator_smooth\n","    series_graph = np.hstack([FCycle, temp_indicator_smooth])\n","    return [series_graph,FCycle,dY_ecdf,C_out,FCyc_sav_unsmooth,weights]\n"],"metadata":{"id":"tFpAUVkPJ96m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"McXq-fC9CrHv","executionInfo":{"status":"ok","timestamp":1730218215016,"user_tz":-60,"elapsed":313,"user":{"displayName":"Moritz Pfeifer","userId":"03033656427288486376"}},"outputId":"5229e148-aa2b-4ba6-961f-163e94c720e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Leipzig/Divergence_Indicator_2_0/Data/Data_Final'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import warnings\n","\n","# Suppress all warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","cpi_data =pd.read_excel(path+'/Data_Overview_29082024.xlsx',sheet_name='CPI')\n","cpi_data_country=cpi_data['Country']\n","for i in range(len(cpi_data_country)):\n","    try:\n","        df_ger_bus = pd.read_excel(path+'/business_cycle_data_final_ERMcut.xlsx',index_col= 'Unnamed: 0',\n","                          sheet_name=cpi_data_country[i])\n","        df_ger_fin = pd.read_excel(path+'/financial_cycle_data_final_ERMcut.xlsx',index_col= 'Unnamed: 0',\n","                          sheet_name=cpi_data_country[i])\n","        df_ger_bus\n","        cpi = pd.read_excel(path+'/CPI.xlsx',index_col='Unnamed: 0')\n","        cpi =cpi[cpi_data_country[i]]\n","\n","        Date_start=cpi_data.loc[cpi_data['Country'] == cpi_data_country[i],'Dates (ERM)'].iloc[0]\n","        df_ger_bus = df_ger_bus[['private consumption', 'unemployment', 'GDP',\n","        'gross fixed capital formation']] # Rearrange\n","        df_ger_fin = df_ger_fin[['credit', 'house price index', 'share price (equity)',\n","        'government bond yield']]\n","        common_index = df_ger_bus.index.intersection(df_ger_fin.index)\n","\n","        # Filter the dataframes to include only the common index\n","        df_ger_bus = df_ger_bus.loc[common_index]\n","        df_ger_fin = df_ger_fin.loc[common_index]\n","        cpi = cpi.loc[common_index]\n","        dates = pd.to_datetime(df_ger_bus.index)\n","        df_ger_bus_year = dates.year\n","        df_ger_fin_year = dates.year\n","        df_ger_bus = df_ger_bus[df_ger_bus_year >= Date_start]\n","        df_ger_fin = df_ger_fin[df_ger_fin_year >= Date_start]\n","\n","\n","        df_ger_bus.dropna(axis=0, how='any', inplace=True)\n","        df_ger_fin.dropna(axis=0, how='any', inplace=True)\n","\n","        common_index = df_ger_bus.index.intersection(df_ger_fin.index)\n","\n","        # Filter the dataframes to include only the common index\n","        df_ger_bus = df_ger_bus.loc[common_index]\n","        df_ger_fin = df_ger_fin.loc[common_index]\n","        cpi = cpi.loc[common_index]\n","        # print(cpi)\n","        # print(len(cpi))\n","        df_ger_bus_1 = df_ger_bus[['private consumption', 'unemployment', 'GDP']].div(cpi, axis=0)*100\n","        df_ger_fin_1= df_ger_fin[['credit', 'house price index', 'share price (equity)']].div(cpi, axis=0)*100\n","        df_ger_bus_2 = df_ger_bus[['gross fixed capital formation']]\n","        df_ger_fin_2= df_ger_fin[['government bond yield']].div((1+df_ger_fin[['government bond yield']]),axis=0)\n","        df_ger_fin_2= df_ger_fin_2.div(cpi,axis=0)*100\n","        df_ger_bus = df_ger_bus_1.join(df_ger_bus_2)\n","        df_ger_fin = df_ger_fin_1.join(df_ger_fin_2)\n","        #print(df_ger_fin)\n","\n","        # Calculate percentage change for df_ger_bus\n","        df_ger_bus = df_ger_bus.pct_change()\n","\n","        # Calculate percentage change for df_ger_fin\n","\n","        if cpi_data_country[i] == 'Malta':\n","            # Exclude 'share price (equity)' for Malta when calculating pct_change\n","            df_ger_fin_except_equity = df_ger_fin.drop(columns=['share price (equity)']).pct_change()\n","            df_ger_fin['share price (equity)'] = df_ger_fin['share price (equity)']  # Keep 'share price (equity)' as is\n","            df_ger_fin.update(df_ger_fin_except_equity)  # Update df_ger_fin with pct_change for other columns\n","        else:\n","            # Apply pct_change to the entire dataframe if not Malta\n","            df_ger_fin = df_ger_fin.pct_change()\n","\n","        #df_ger_fin = df_ger_fin.pct_change()\n","\n","        df_ger_bus_ = df_ger_bus.iloc[1:-1,:]\n","        df_ger_fin_ = df_ger_fin.iloc[1:-1,:]\n","        df_ger_bus_.replace([np.inf, -np.inf], 0, inplace=True)\n","        df_ger_fin_.replace([np.inf, -np.inf], 0, inplace=True)\n","        print(df_ger_bus_.isna().sum().sum())\n","        print(df_ger_fin_.isna().sum().sum())\n","\n","        dates = pd.to_datetime(df_ger_bus_.index)\n","        df_ger_bus_= df_ger_bus_.reset_index(drop=True)\n","        df_ger_fin_= df_ger_fin_.reset_index(drop=True)\n","        df_ger_bus_np = df_ger_bus_.to_numpy()\n","        df_ger_fin_np = df_ger_fin_.to_numpy()\n","\n","        centered_data_bus = df_ger_bus_np - df_ger_bus_np.mean()\n","        centered_data_fin = df_ger_fin_np - df_ger_fin_np.mean()\n","\n","        dY_bus = df_ger_bus_np\n","        dY_fin = df_ger_fin_np\n","\n","        co = [20]            # number of countries\n","        n_co = len(co)\n","        T = dY_bus.shape[0]\n","\n","\n","        fdY_bus = fn_bpass_all(centered_data_bus,2,200,0,0,0)\n","        fdY_fin = fn_bpass_all(centered_data_fin,2,200,0,0,0)\n","\n","        q_low = 5             # lower bound for spectral density, put 0 for lowest (10=benchmark)\n","        q_high = 200          # upper bound for spectral density, put 0 for highest (0 = benchmark)\n","        factor = 8            # precision of spectral density estimates\n","        N = 1                 # number of peaks to select\n","        pct = 0.67            # percentage rule for the area of power cohesion to select\n","        corr_rol = 1          # composite Cycle: if 1 - rolling correlations; 0 - linear average\n","        sgn_res = 1           # sgn restrictions CISS: emphasises positively related movements\n","        idx_fc = [1, 1, 1, 1]\n","        idx_bc = [1, 1, 1, 1]\n","\n","        grid, step = fn_grid(q_low, q_high, 16)\n","        # FC broad\n","        FCycle = np.zeros((T, n_co))\n","        FCycle_rt = np.zeros((T, n_co))\n","        FCycle_unsmooth = np.zeros((T, n_co))\n","        FCycle_unsmooth_rt = np.zeros((T, n_co))\n","        series_graph_fc = np.zeros((T, n_co * 5))\n","        time_varying_weights =  np.zeros((T, n_co * 4))\n","        # FC narrow\n","        FCycle_n = np.zeros((T, n_co))\n","        FCycle_n_rt = np.zeros((T, n_co))\n","        series_graph_fc_n = np.zeros((T, n_co * 3))\n","        FCycle_unsmooth_n = np.zeros((T, n_co))\n","        FCycle_unsmooth_n_rt = np.zeros((T, n_co))\n","        # Business Cycle\n","        BCycle = np.zeros((T, n_co))\n","        BCycle_rt = np.zeros((T, n_co))\n","        series_graph_bc = np.zeros((T, n_co * 5))\n","        BCycle_unsmooth = np.zeros((T, n_co))\n","        BCycle_unsmooth_rt = np.zeros((T, n_co))\n","\n","        fdY_fin = np.array(fdY_fin)\n","        print(fdY_fin.shape)\n","        fdY_bus = np.array(fdY_bus)\n","        dY_fin = np.array(dY_fin)\n","        dY_bus = np.array(dY_bus)\n","\n","        # Initialize arrays to store peaks\n","        peak_fc = np.zeros((1, 1))\n","        peak_fc_n = np.zeros((1, 1))\n","        peak_bc = np.zeros((1, 1))\n","\n","        # Loop across countries (here, only the US)\n","        for jip in range(len(co)):\n","            dY_f = fdY_fin  # Data for financial cycles\n","            dY_b = fdY_bus  # Data for business cycles\n","            dY_f_rt = dY_fin  # Real-time data for financial cycles\n","            dY_b_rt = dY_bus  # Real-time data for business cycles\n","\n","            # Power cohesion calculations\n","            pw_coh_fc, f_hats_fc = pw_cohesion(dY_f, grid, factor,idx_fc )\n","            pw_coh_fc_n, f_hats_fc_n = pw_cohesion(dY_f, grid, factor, [1, 1, 0, 0])\n","            pw_coh_bc, f_hats_bc = pw_cohesion(dY_b, grid, factor, idx_bc)\n","\n","            # Store peak frequencies\n","            peak_fc = np.zeros((1, N))\n","            peak_fc_n = np.zeros((1, N))\n","            peak_bc = np.zeros((1, N))\n","\n","\n","        temp1, _, _ = extrema(pw_coh_fc)\n","        temp15, _, _ = extrema(pw_coh_fc_n)\n","        temp2, _, _ = extrema(pw_coh_bc)\n","\n","        temp1_indices = temp1[:, 0].astype(int)[temp1[:, 0] < len(pw_coh_fc)]\n","        temp15_indices = temp15[:, 0].astype(int)[temp15[:, 0] < len(pw_coh_fc_n)]\n","        temp2_indices = temp2[:, 0].astype(int)[temp2[:, 0] < len(pw_coh_bc)]\n","\n","        # Finding the highest N maxima\n","        # For financial cycles (broad)\n","        sort_index = np.argsort(-pw_coh_fc[temp1_indices])  # Sorting in descending order\n","        max_index = sort_index[:N]\n","        peak_fc = (np.pi / 2) / grid[temp1_indices[max_index]]\n","\n","        # For financial cycles (narrow)\n","        sort_index = np.argsort(-pw_coh_fc_n[temp15_indices])\n","        max_index = sort_index[:N]\n","        peak_fc_n = (np.pi / 2) / grid[temp15_indices[max_index]]\n","\n","        # For business cycles\n","        sort_index = np.argsort(-pw_coh_bc[temp2_indices])\n","        max_index = sort_index[:N]\n","        peak_bc = (np.pi / 2) / grid[temp2_indices[max_index]]\n","\n","        # Derive area and distance interval\n","        min_distance_fc = fn_integral_min_dist(pw_coh_fc, peak_fc[0], grid, pct)\n","        min_distance_fc_n = fn_integral_min_dist(pw_coh_fc_n, peak_fc_n[0], grid, pct)\n","        min_distance_bc = fn_integral_min_dist(pw_coh_bc, peak_bc[0], grid, pct)\n","\n","\n","        # Store freq windows and peaks\n","        table_freqband_fcycle = np.zeros((len(co), 3))  # Update dimensions as needed\n","        table_freqband_fcycle_n = np.zeros((len(co), 3))\n","        table_freqband_bcycle = np.zeros((len(co), 3))\n","\n","        table_freqband_fcycle[jip] = [min_distance_fc[2], peak_fc[0], min_distance_fc[3]]\n","        table_freqband_fcycle_n[jip] = [min_distance_fc_n[2], peak_fc_n[0], min_distance_fc_n[3]]\n","        table_freqband_bcycle[jip] = [min_distance_bc[2], peak_bc[0], min_distance_bc[3]]\n","\n","        # Financial Cycle: Broad\n","        jip=1\n","        [series_graph_fc[:, 5*jip-5:5*jip],FCycle[:],_,_,FCycle_unsmooth[:],time_varying_weights[:,(jip-1)*4:(jip-1)*4+4]] = fn_cycle_extract_ecdf_aggr(dY_f, idx_fc, min_distance_fc, corr_rol, sgn_res)\n","        # Financial Cycle: Narrow\n","        idx_fcn=[1, 1, 0, 0]\n","        [series_graph_fc_n[:, 3*jip-3:3*jip],FCycle_n[:],_, _,FCycle_unsmooth_n[:],_] = fn_cycle_extract_ecdf_aggr(dY_f, idx_fcn, min_distance_fc_n, 0, sgn_res)\n","\n","        # Business Cycle\n","        [series_graph_bc[:, 5*jip-5:5*jip],BCycle[:],_,_,BCycle_unsmooth[:],_] = fn_cycle_extract_ecdf_aggr(dY_b, idx_bc, min_distance_bc, corr_rol, sgn_res)\n","\n","        # plt.figure(figsize=(10, 6))\n","        plt.subplots(figsize=(30,6))\n","        plt.suptitle(cpi_data_country[i],fontsize=20)\n","        plt.subplot(1, 3, 1)\n","        sns.lineplot(x=dates, y=FCycle_unsmooth[:].flatten(), label='FCycle Unsmooth')\n","        sns.lineplot(x=dates, y=FCycle[:].flatten(), label='FCycle')\n","        plt.axhline(0.5, color='gray', linestyle='--', label='Zero Line')\n","        plt.xlabel('Year')\n","        plt.ylabel('Deviation from historical median growth')\n","        # plt.show()\n","\n","        plt.subplot(1, 3, 2)\n","        #plt.figure(figsize=(8, 4))\n","        sns.lineplot(x=dates, y=FCycle_unsmooth_n[:].flatten(), label='FCycle Unsmooth_n')\n","        sns.lineplot(x=dates, y=FCycle_n[:].flatten(), label='FCycle_n')\n","        plt.axhline(0.5, color='gray', linestyle='--', label='Zero Line')\n","        plt.xlabel('Year')\n","        plt.ylabel('Deviation from historical median growth')\n","        # plt.show()\n","\n","        plt.subplot(1, 3, 3)\n","        #plt.figure(figsize=(8, 4))\n","        sns.lineplot(x=dates, y=BCycle_unsmooth[:].flatten(), label='BCycle Unsmooth')\n","        sns.lineplot(x=dates, y=BCycle[:].flatten(), label='BCycle')\n","        plt.axhline(0.5, color='gray', linestyle='--', label='Zero Line')\n","        plt.xlabel('Year')\n","        plt.ylabel('Deviation from historical median growth')\n","        plt.show()\n","\n","        cycle_data = pd.DataFrame({\n","            'FCycle_unsmooth': FCycle_unsmooth[:].flatten(),\n","            'FCycle': FCycle[:].flatten(),\n","            'FCycle_unsmooth_n': FCycle_unsmooth_n[:].flatten(),\n","            'FCycle_n': FCycle_n[:].flatten(),\n","            'BCycle_unsmooth': BCycle_unsmooth[:].flatten(),\n","            'BCycle': BCycle[:].flatten(),\n","        }, index=dates)\n","\n","        # Create the Excel file path\n","        #excel_file_path = os.path.join(path, 'Country_Cycle_Data', f'{cpi_data_country[i]}_Cycle_Data.xlsx')\n","\n","        # Write the DataFrame to an Excel file with the index as dates\n","        with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='a' if i > 0 else 'w') as writer:\n","            cycle_data.to_excel(writer, sheet_name=cpi_data_country[i], index=True)\n","\n","    except Exception as e:\n","        print(f\"Error processing {cpi_data_country[i]}: {e}\")\n","        continue\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tZTqU8XZVyCiJnVqoRDsHZwNuGlnzppE"},"id":"WEa2EfD1ISp8","outputId":"d8151682-6a3f-424f-d7d7-3ff8c17fa3ff","executionInfo":{"status":"ok","timestamp":1730285523370,"user_tz":-60,"elapsed":257691,"user":{"displayName":"Moritz Pfeifer","userId":"03033656427288486376"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}